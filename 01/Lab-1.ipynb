{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# âœ¨ Text Normalization Tasks ğŸ“–\n",
    "\n",
    "Text normalization is an essential **preprocessing step** in NLP. It includes:\n",
    "\n",
    "- ğŸ”¹ **Tokenization**: Splitting text into words or subwords.  \n",
    "- ğŸ”¹ **Segmentation**: Dividing text into meaningful units such as sentences or phrases.  \n",
    "- ğŸ”¹ **Stemming**: Reducing words to their root form by removing affixes.  \n",
    "- ğŸ”¹ **Lemmatization**: Converting words to their base dictionary form.  "
   ],
   "id": "7aca1de6d065c151"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **ğŸ”¹ Arabic Text**\n",
    "> Ø¥Ù† Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ Ø³ÙˆÙ ÙŠØªÙˆÙ‚Ù Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø£Ù…Ø§Ù… Ø¹Ù…Ù„ÙŠØ© ÙŠÙˆÙ… **Ù¦ Ø£ÙƒØªÙˆØ¨Ø± Ù¡Ù©Ù§Ù£**ØŒ ÙˆÙ„Ø³Øª Ø£ØªØ¬Ø§ÙˆØ² Ø¥Ø°Ø§ Ù‚Ù„Øª Ø£Ù† Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ Ø³ÙˆÙ ÙŠØªÙˆÙ‚Ù Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø¨Ø§Ù„ÙØ­Øµ ÙˆØ§Ù„Ø¯Ø±Ø³ Ø£Ù…Ø§Ù… Ø¹Ù…Ù„ÙŠØ© ÙŠÙˆÙ… **Ù¦ Ø£ÙƒØªÙˆØ¨Ø± Ø³Ù†Ø© Ù§Ù£**ØŒ Ø­ÙŠÙ† ØªÙ…ÙƒÙ†Øª Ø§Ù„Ù‚ÙˆØ§Øª Ø§Ù„Ù…Ø³Ù„Ø­Ø© Ø§Ù„Ù…ØµØ±ÙŠØ© Ù…Ù† Ø§Ù‚ØªØ­Ø§Ù… Ù…Ø§Ù†Ø¹ **Ù‚Ù†Ø§Ø© Ø§Ù„Ø³ÙˆÙŠØ³** Ø§Ù„ØµØ¹Ø¨ ÙˆØ§Ø¬ØªÙŠØ§Ø² **Ø®Ø· Ø¨Ø§Ø±Ù„ÙŠÙ** Ø§Ù„Ù…Ù†ÙŠØ¹ ÙˆØ¥Ù‚Ø§Ù…Ø© Ø±Ø¤ÙˆØ³ Ø¬Ø³ÙˆØ± Ù„Ù‡Ø§ Ø¨Ø¹Ø¯ Ø£Ù† Ø£ÙÙ‚Ø¯Øª Ø§Ù„Ø¹Ø¯Ùˆ ØªÙˆØ§Ø²Ù†Ù‡ ÙÙŠ **6 Ø³Ø§Ø¹Ø§Øª**ØŒ Ù„Ù‚Ø¯ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø© ÙƒØ¨ÙŠØ±Ø© ÙˆØ§Ù„ØªØ¶Ø­ÙŠØ§Øª Ø¹Ø¸ÙŠÙ…Ø© Ù„Ù…Ø¹Ø±ÙƒØ© **Ù¦ Ø£ÙƒØªÙˆØ¨Ø±** Ø®Ù„Ø§Ù„ **Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø³Øª Ø§Ù„Ø£ÙˆÙ„Ù‰** Ù…Ù† Ø­Ø±Ø¨Ù†Ø§ ÙƒØ§Ù†Øª Ù‡Ø§Ø¦Ù„Ø© ÙÙ‚Ø¯ Ø§Ù„Ø¹Ø¯Ùˆ ØªÙˆØ§Ø²Ù†Ù‡ Ø¥Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ù„Ø­Ø¸Ø©.\n"
   ],
   "id": "3dfcc28372cdb957"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aabe4a5c"
   },
   "source": [
    "## **Q1: Tokenization Using Three Methods**\n",
    "\n",
    "Tokenization is the process of breaking down a **phrase, sentence, paragraph, or entire document** into smaller units called **tokens**. These tokens can be individual **words, subwords, or punctuation marks**.\n",
    "\n",
    "For this task, apply **three different tokenization methods**:\n",
    "1. **Two traditional Python-based methods**\n",
    "2. **One NLP library-based method**\n"
   ],
   "id": "325fb1b363c0fdf9"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "9870a17a",
    "outputId": "c155c580-d966-408c-fc1f-cf6f89dc295e",
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:15.703569Z",
     "start_time": "2025-02-15T22:50:15.694255Z"
    }
   },
   "source": "text = ' Ø¥Ù† Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ Ø³ÙˆÙ ÙŠØªÙˆÙ‚Ù Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø£Ù…Ø§Ù… Ø¹Ù…Ù„ÙŠØ© ÙŠÙˆÙ… Ù¦ Ø£ÙƒØªÙˆØ¨Ø± Ù¡Ù©Ù§Ù£, ÙˆÙ„Ø³Øª Ø§ØªØ¬Ø§ÙˆØ² Ø§Ø°Ø§ Ù‚Ù„Øª Ø£Ù† Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ Ø³ÙˆÙ ÙŠØªÙˆÙ‚Ù Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø¨Ø§Ù„ÙØ­Øµ ÙˆØ§Ù„Ø¯Ø±Ø³ Ø£Ù…Ø§Ù… Ø¹Ù…Ù„ÙŠØ© ÙŠÙˆÙ… Ù¦ Ø£ÙƒØªÙˆØ¨Ø± Ø³Ù†Ø© Ù§Ù£, Ø­ÙŠÙ† ØªÙ…ÙƒÙ†Øª Ø§Ù„Ù‚ÙˆØ§Øª Ø§Ù„Ù…Ø³Ù„Ø­Ø© Ø§Ù„Ù…ØµØ±ÙŠØ© Ù…Ù† Ø¥Ù‚ØªØ­Ø§Ù… Ù…Ø§Ù†Ø¹ Ù‚Ù†Ø§Ø© Ø§Ù„Ø³ÙˆÙŠØ³ Ø§Ù„ØµØ¹Ø¨ ÙˆØ§Ø¬ØªÙŠØ§Ø² Ø®Ø· Ø¨Ø§Ø±Ù„ÙŠÙ Ø§Ù„Ù…Ù†ÙŠØ¹ ÙˆØ¥Ù‚Ø§Ù…Ø© Ø±Ø¤Ø³ Ø¬Ø³ÙˆØ± Ù„Ù‡Ø§ Ø¨Ø¹Ø¯ Ø£Ù† Ø£ÙÙ‚Ø¯Øª Ø§Ù„Ø¹Ø¯Ùˆ ØªÙˆØ§Ø²Ù†Ù‡ ÙÙ‰ 6 Ø³Ø§Ø¹Ø§Øª, Ù„Ù‚Ø¯ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø© ÙƒØ¨ÙŠØ±Ø© ÙˆØ§Ù„ØªØ¶Ø­ÙŠØ§Øª Ø¹Ø¸ÙŠÙ…Ø© Ù„Ù…Ø¹Ø±ÙƒØ© Ù¦ Ø£ÙƒØªÙˆØ¨Ø± Ø®Ù„Ø§Ù„ Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø³Øª Ø§Ù„Ø£ÙˆÙ„ÙŠ Ù…Ù† Ø­Ø±Ø¨Ù†Ø§ ÙƒØ§Ù†Øª Ù‡Ø§Ø¦Ù„Ø© ÙÙ‚Ø¯ Ø§Ù„Ø¹Ø¯Ùˆ ØªÙˆØ§Ø²Ù†Ù‡ Ø¥Ù„ÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù„Ø­Ø¸Ø©'",
   "id": "e55973fec6b869fe",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "cc744678",
    "outputId": "1e88e2a2-2a40-4aa5-c3a4-3c3a80bad2be",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:15.732963Z",
     "start_time": "2025-02-15T22:50:15.723522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocessing\n",
    "import re\n",
    "pre_processed = re.sub(r'[^\\w\\s]', '', text) # Keep only whitespaces & words\n",
    "pre_processed"
   ],
   "id": "165b13ee7a308e88",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Ø¥Ù† Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ Ø³ÙˆÙ ÙŠØªÙˆÙ‚Ù Ø·ÙˆÙŠÙ„Ø§ Ø£Ù…Ø§Ù… Ø¹Ù…Ù„ÙŠØ© ÙŠÙˆÙ… Ù¦ Ø£ÙƒØªÙˆØ¨Ø± Ù¡Ù©Ù§Ù£ ÙˆÙ„Ø³Øª Ø§ØªØ¬Ø§ÙˆØ² Ø§Ø°Ø§ Ù‚Ù„Øª Ø£Ù† Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ Ø³ÙˆÙ ÙŠØªÙˆÙ‚Ù Ø·ÙˆÙŠÙ„Ø§ Ø¨Ø§Ù„ÙØ­Øµ ÙˆØ§Ù„Ø¯Ø±Ø³ Ø£Ù…Ø§Ù… Ø¹Ù…Ù„ÙŠØ© ÙŠÙˆÙ… Ù¦ Ø£ÙƒØªÙˆØ¨Ø± Ø³Ù†Ø© Ù§Ù£ Ø­ÙŠÙ† ØªÙ…ÙƒÙ†Øª Ø§Ù„Ù‚ÙˆØ§Øª Ø§Ù„Ù…Ø³Ù„Ø­Ø© Ø§Ù„Ù…ØµØ±ÙŠØ© Ù…Ù† Ø¥Ù‚ØªØ­Ø§Ù… Ù…Ø§Ù†Ø¹ Ù‚Ù†Ø§Ø© Ø§Ù„Ø³ÙˆÙŠØ³ Ø§Ù„ØµØ¹Ø¨ ÙˆØ§Ø¬ØªÙŠØ§Ø² Ø®Ø· Ø¨Ø§Ø±Ù„ÙŠÙ Ø§Ù„Ù…Ù†ÙŠØ¹ ÙˆØ¥Ù‚Ø§Ù…Ø© Ø±Ø¤Ø³ Ø¬Ø³ÙˆØ± Ù„Ù‡Ø§ Ø¨Ø¹Ø¯ Ø£Ù† Ø£ÙÙ‚Ø¯Øª Ø§Ù„Ø¹Ø¯Ùˆ ØªÙˆØ§Ø²Ù†Ù‡ ÙÙ‰ 6 Ø³Ø§Ø¹Ø§Øª Ù„Ù‚Ø¯ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø© ÙƒØ¨ÙŠØ±Ø© ÙˆØ§Ù„ØªØ¶Ø­ÙŠØ§Øª Ø¹Ø¸ÙŠÙ…Ø© Ù„Ù…Ø¹Ø±ÙƒØ© Ù¦ Ø£ÙƒØªÙˆØ¨Ø± Ø®Ù„Ø§Ù„ Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø³Øª Ø§Ù„Ø£ÙˆÙ„ÙŠ Ù…Ù† Ø­Ø±Ø¨Ù†Ø§ ÙƒØ§Ù†Øª Ù‡Ø§Ø¦Ù„Ø© ÙÙ‚Ø¯ Ø§Ù„Ø¹Ø¯Ùˆ ØªÙˆØ§Ø²Ù†Ù‡ Ø¥Ù„ÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù„Ø­Ø¸Ø©'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d02ed2f"
   },
   "source": [
    "## **ğŸ”¸ Tokenization Methods**\n",
    "1ï¸âƒ£ **Pythonâ€™s Built-in `split()` Method** (Basic, whitespace-based tokenization)  \n",
    "2ï¸âƒ£ **Regular Expressions (`re.findall(PATTERN, TEXT)`,`re.split()`)**  \n",
    "3ï¸âƒ£ **NLP Library (`nltk.word_tokenize` for English, `Farasa` for Arabic)** (Context-aware, language-specific tokenization)\n"
   ],
   "id": "90b93299e4334fb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **1ï¸âƒ£ Using Pythonâ€™s `split()` Method (Traditional Method)**\n",
    "Python's built-in `split()` method can be used to tokenize text **based on spaces**. However, it does not handle punctuation or special cases."
   ],
   "id": "3f28a8b0bc968c87"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9ff7cc5",
    "outputId": "a96952f7-cc17-4a4d-d363-ce0e3c25efad",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:15.989553Z",
     "start_time": "2025-02-15T22:50:15.981097Z"
    }
   },
   "source": [
    "tokens_1 = pre_processed.split()\n",
    "tokens_1"
   ],
   "id": "8f5d0b349acc3e26",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ø¥Ù†',\n",
       " 'Ø§Ù„ØªØ§Ø±ÙŠØ®',\n",
       " 'Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ',\n",
       " 'Ø³ÙˆÙ',\n",
       " 'ÙŠØªÙˆÙ‚Ù',\n",
       " 'Ø·ÙˆÙŠÙ„Ø§',\n",
       " 'Ø£Ù…Ø§Ù…',\n",
       " 'Ø¹Ù…Ù„ÙŠØ©',\n",
       " 'ÙŠÙˆÙ…',\n",
       " 'Ù¦',\n",
       " 'Ø£ÙƒØªÙˆØ¨Ø±',\n",
       " 'Ù¡Ù©Ù§Ù£',\n",
       " 'ÙˆÙ„Ø³Øª',\n",
       " 'Ø§ØªØ¬Ø§ÙˆØ²',\n",
       " 'Ø§Ø°Ø§',\n",
       " 'Ù‚Ù„Øª',\n",
       " 'Ø£Ù†',\n",
       " 'Ø§Ù„ØªØ§Ø±ÙŠØ®',\n",
       " 'Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ',\n",
       " 'Ø³ÙˆÙ',\n",
       " 'ÙŠØªÙˆÙ‚Ù',\n",
       " 'Ø·ÙˆÙŠÙ„Ø§',\n",
       " 'Ø¨Ø§Ù„ÙØ­Øµ',\n",
       " 'ÙˆØ§Ù„Ø¯Ø±Ø³',\n",
       " 'Ø£Ù…Ø§Ù…',\n",
       " 'Ø¹Ù…Ù„ÙŠØ©',\n",
       " 'ÙŠÙˆÙ…',\n",
       " 'Ù¦',\n",
       " 'Ø£ÙƒØªÙˆØ¨Ø±',\n",
       " 'Ø³Ù†Ø©',\n",
       " 'Ù§Ù£',\n",
       " 'Ø­ÙŠÙ†',\n",
       " 'ØªÙ…ÙƒÙ†Øª',\n",
       " 'Ø§Ù„Ù‚ÙˆØ§Øª',\n",
       " 'Ø§Ù„Ù…Ø³Ù„Ø­Ø©',\n",
       " 'Ø§Ù„Ù…ØµØ±ÙŠØ©',\n",
       " 'Ù…Ù†',\n",
       " 'Ø¥Ù‚ØªØ­Ø§Ù…',\n",
       " 'Ù…Ø§Ù†Ø¹',\n",
       " 'Ù‚Ù†Ø§Ø©',\n",
       " 'Ø§Ù„Ø³ÙˆÙŠØ³',\n",
       " 'Ø§Ù„ØµØ¹Ø¨',\n",
       " 'ÙˆØ§Ø¬ØªÙŠØ§Ø²',\n",
       " 'Ø®Ø·',\n",
       " 'Ø¨Ø§Ø±Ù„ÙŠÙ',\n",
       " 'Ø§Ù„Ù…Ù†ÙŠØ¹',\n",
       " 'ÙˆØ¥Ù‚Ø§Ù…Ø©',\n",
       " 'Ø±Ø¤Ø³',\n",
       " 'Ø¬Ø³ÙˆØ±',\n",
       " 'Ù„Ù‡Ø§',\n",
       " 'Ø¨Ø¹Ø¯',\n",
       " 'Ø£Ù†',\n",
       " 'Ø£ÙÙ‚Ø¯Øª',\n",
       " 'Ø§Ù„Ø¹Ø¯Ùˆ',\n",
       " 'ØªÙˆØ§Ø²Ù†Ù‡',\n",
       " 'ÙÙ‰',\n",
       " '6',\n",
       " 'Ø³Ø§Ø¹Ø§Øª',\n",
       " 'Ù„Ù‚Ø¯',\n",
       " 'ÙƒØ§Ù†Øª',\n",
       " 'Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø©',\n",
       " 'ÙƒØ¨ÙŠØ±Ø©',\n",
       " 'ÙˆØ§Ù„ØªØ¶Ø­ÙŠØ§Øª',\n",
       " 'Ø¹Ø¸ÙŠÙ…Ø©',\n",
       " 'Ù„Ù…Ø¹Ø±ÙƒØ©',\n",
       " 'Ù¦',\n",
       " 'Ø£ÙƒØªÙˆØ¨Ø±',\n",
       " 'Ø®Ù„Ø§Ù„',\n",
       " 'Ø§Ù„Ø³Ø§Ø¹Ø§Øª',\n",
       " 'Ø§Ù„Ø³Øª',\n",
       " 'Ø§Ù„Ø£ÙˆÙ„ÙŠ',\n",
       " 'Ù…Ù†',\n",
       " 'Ø­Ø±Ø¨Ù†Ø§',\n",
       " 'ÙƒØ§Ù†Øª',\n",
       " 'Ù‡Ø§Ø¦Ù„Ø©',\n",
       " 'ÙÙ‚Ø¯',\n",
       " 'Ø§Ù„Ø¹Ø¯Ùˆ',\n",
       " 'ØªÙˆØ§Ø²Ù†Ù‡',\n",
       " 'Ø¥Ù„ÙŠ',\n",
       " 'Ù‡Ø°Ù‡',\n",
       " 'Ø§Ù„Ù„Ø­Ø¸Ø©']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f68764a1"
   },
   "source": [
    "## **2ï¸âƒ£ Using Regular Expressions**\n",
    "We can use **regular expressions (`re`)** to split text.\n"
   ],
   "id": "d572753a42560838"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8a9333a5",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:16.031445Z",
     "start_time": "2025-02-15T22:50:16.024047Z"
    }
   },
   "source": [
    "PATTERN = r\"\\w+\"\n",
    "tokens_2 = re.findall(PATTERN, pre_processed)\n",
    "tokens_2"
   ],
   "id": "4c88f639656460fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ø¥Ù†',\n",
       " 'Ø§Ù„ØªØ§Ø±ÙŠØ®',\n",
       " 'Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ',\n",
       " 'Ø³ÙˆÙ',\n",
       " 'ÙŠØªÙˆÙ‚Ù',\n",
       " 'Ø·ÙˆÙŠÙ„Ø§',\n",
       " 'Ø£Ù…Ø§Ù…',\n",
       " 'Ø¹Ù…Ù„ÙŠØ©',\n",
       " 'ÙŠÙˆÙ…',\n",
       " 'Ù¦',\n",
       " 'Ø£ÙƒØªÙˆØ¨Ø±',\n",
       " 'Ù¡Ù©Ù§Ù£',\n",
       " 'ÙˆÙ„Ø³Øª',\n",
       " 'Ø§ØªØ¬Ø§ÙˆØ²',\n",
       " 'Ø§Ø°Ø§',\n",
       " 'Ù‚Ù„Øª',\n",
       " 'Ø£Ù†',\n",
       " 'Ø§Ù„ØªØ§Ø±ÙŠØ®',\n",
       " 'Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ',\n",
       " 'Ø³ÙˆÙ',\n",
       " 'ÙŠØªÙˆÙ‚Ù',\n",
       " 'Ø·ÙˆÙŠÙ„Ø§',\n",
       " 'Ø¨Ø§Ù„ÙØ­Øµ',\n",
       " 'ÙˆØ§Ù„Ø¯Ø±Ø³',\n",
       " 'Ø£Ù…Ø§Ù…',\n",
       " 'Ø¹Ù…Ù„ÙŠØ©',\n",
       " 'ÙŠÙˆÙ…',\n",
       " 'Ù¦',\n",
       " 'Ø£ÙƒØªÙˆØ¨Ø±',\n",
       " 'Ø³Ù†Ø©',\n",
       " 'Ù§Ù£',\n",
       " 'Ø­ÙŠÙ†',\n",
       " 'ØªÙ…ÙƒÙ†Øª',\n",
       " 'Ø§Ù„Ù‚ÙˆØ§Øª',\n",
       " 'Ø§Ù„Ù…Ø³Ù„Ø­Ø©',\n",
       " 'Ø§Ù„Ù…ØµØ±ÙŠØ©',\n",
       " 'Ù…Ù†',\n",
       " 'Ø¥Ù‚ØªØ­Ø§Ù…',\n",
       " 'Ù…Ø§Ù†Ø¹',\n",
       " 'Ù‚Ù†Ø§Ø©',\n",
       " 'Ø§Ù„Ø³ÙˆÙŠØ³',\n",
       " 'Ø§Ù„ØµØ¹Ø¨',\n",
       " 'ÙˆØ§Ø¬ØªÙŠØ§Ø²',\n",
       " 'Ø®Ø·',\n",
       " 'Ø¨Ø§Ø±Ù„ÙŠÙ',\n",
       " 'Ø§Ù„Ù…Ù†ÙŠØ¹',\n",
       " 'ÙˆØ¥Ù‚Ø§Ù…Ø©',\n",
       " 'Ø±Ø¤Ø³',\n",
       " 'Ø¬Ø³ÙˆØ±',\n",
       " 'Ù„Ù‡Ø§',\n",
       " 'Ø¨Ø¹Ø¯',\n",
       " 'Ø£Ù†',\n",
       " 'Ø£ÙÙ‚Ø¯Øª',\n",
       " 'Ø§Ù„Ø¹Ø¯Ùˆ',\n",
       " 'ØªÙˆØ§Ø²Ù†Ù‡',\n",
       " 'ÙÙ‰',\n",
       " '6',\n",
       " 'Ø³Ø§Ø¹Ø§Øª',\n",
       " 'Ù„Ù‚Ø¯',\n",
       " 'ÙƒØ§Ù†Øª',\n",
       " 'Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø©',\n",
       " 'ÙƒØ¨ÙŠØ±Ø©',\n",
       " 'ÙˆØ§Ù„ØªØ¶Ø­ÙŠØ§Øª',\n",
       " 'Ø¹Ø¸ÙŠÙ…Ø©',\n",
       " 'Ù„Ù…Ø¹Ø±ÙƒØ©',\n",
       " 'Ù¦',\n",
       " 'Ø£ÙƒØªÙˆØ¨Ø±',\n",
       " 'Ø®Ù„Ø§Ù„',\n",
       " 'Ø§Ù„Ø³Ø§Ø¹Ø§Øª',\n",
       " 'Ø§Ù„Ø³Øª',\n",
       " 'Ø§Ù„Ø£ÙˆÙ„ÙŠ',\n",
       " 'Ù…Ù†',\n",
       " 'Ø­Ø±Ø¨Ù†Ø§',\n",
       " 'ÙƒØ§Ù†Øª',\n",
       " 'Ù‡Ø§Ø¦Ù„Ø©',\n",
       " 'ÙÙ‚Ø¯',\n",
       " 'Ø§Ù„Ø¹Ø¯Ùˆ',\n",
       " 'ØªÙˆØ§Ø²Ù†Ù‡',\n",
       " 'Ø¥Ù„ÙŠ',\n",
       " 'Ù‡Ø°Ù‡',\n",
       " 'Ø§Ù„Ù„Ø­Ø¸Ø©']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cd43a3e"
   },
   "source": [
    "## **3ï¸âƒ£ Using NLP Libraries (`nltk.word_tokenize`)**\n",
    "The **NLTK tokenizer** provides a **smart way** to split words while handling punctuation **and contractions** properly."
   ],
   "id": "81b525c81f541758"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cfab0cc1",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:16.835764Z",
     "start_time": "2025-02-15T22:50:16.100666Z"
    }
   },
   "source": [
    "import nltk\n",
    "# Punkt is a pretrained model that helps in splitting text into sentences and words.\n",
    "# It supports multiple languages, including English and Arabic.\n",
    "# The \"punkt\" package consists in unsafe pickles, so it is deprecated and not used in NLTK 3.8.2. It is replaced by \"punkt_tab\".\n",
    "\n",
    "# nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ],
   "id": "135a6e89a98b1543",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mmaba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mmaba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8jCr3eI_ZNvG",
    "outputId": "44ab8cab-4383-4527-a57e-d6dac7379cb8",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:16.856275Z",
     "start_time": "2025-02-15T22:50:16.851956Z"
    }
   },
   "source": [
    "from nltk.tokenize import word_tokenize"
   ],
   "id": "8b637d53abe206a3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "220930ec",
    "outputId": "87c67cef-fe3e-469d-8be6-bc66cd9d2b36",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:16.933669Z",
     "start_time": "2025-02-15T22:50:16.893221Z"
    }
   },
   "source": [
    "tokens_3 = word_tokenize(pre_processed)\n",
    "tokens_3"
   ],
   "id": "34b0bee7b0185a6e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ø¥Ù†',\n",
       " 'Ø§Ù„ØªØ§Ø±ÙŠØ®',\n",
       " 'Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ',\n",
       " 'Ø³ÙˆÙ',\n",
       " 'ÙŠØªÙˆÙ‚Ù',\n",
       " 'Ø·ÙˆÙŠÙ„Ø§',\n",
       " 'Ø£Ù…Ø§Ù…',\n",
       " 'Ø¹Ù…Ù„ÙŠØ©',\n",
       " 'ÙŠÙˆÙ…',\n",
       " 'Ù¦',\n",
       " 'Ø£ÙƒØªÙˆØ¨Ø±',\n",
       " 'Ù¡Ù©Ù§Ù£',\n",
       " 'ÙˆÙ„Ø³Øª',\n",
       " 'Ø§ØªØ¬Ø§ÙˆØ²',\n",
       " 'Ø§Ø°Ø§',\n",
       " 'Ù‚Ù„Øª',\n",
       " 'Ø£Ù†',\n",
       " 'Ø§Ù„ØªØ§Ø±ÙŠØ®',\n",
       " 'Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ',\n",
       " 'Ø³ÙˆÙ',\n",
       " 'ÙŠØªÙˆÙ‚Ù',\n",
       " 'Ø·ÙˆÙŠÙ„Ø§',\n",
       " 'Ø¨Ø§Ù„ÙØ­Øµ',\n",
       " 'ÙˆØ§Ù„Ø¯Ø±Ø³',\n",
       " 'Ø£Ù…Ø§Ù…',\n",
       " 'Ø¹Ù…Ù„ÙŠØ©',\n",
       " 'ÙŠÙˆÙ…',\n",
       " 'Ù¦',\n",
       " 'Ø£ÙƒØªÙˆØ¨Ø±',\n",
       " 'Ø³Ù†Ø©',\n",
       " 'Ù§Ù£',\n",
       " 'Ø­ÙŠÙ†',\n",
       " 'ØªÙ…ÙƒÙ†Øª',\n",
       " 'Ø§Ù„Ù‚ÙˆØ§Øª',\n",
       " 'Ø§Ù„Ù…Ø³Ù„Ø­Ø©',\n",
       " 'Ø§Ù„Ù…ØµØ±ÙŠØ©',\n",
       " 'Ù…Ù†',\n",
       " 'Ø¥Ù‚ØªØ­Ø§Ù…',\n",
       " 'Ù…Ø§Ù†Ø¹',\n",
       " 'Ù‚Ù†Ø§Ø©',\n",
       " 'Ø§Ù„Ø³ÙˆÙŠØ³',\n",
       " 'Ø§Ù„ØµØ¹Ø¨',\n",
       " 'ÙˆØ§Ø¬ØªÙŠØ§Ø²',\n",
       " 'Ø®Ø·',\n",
       " 'Ø¨Ø§Ø±Ù„ÙŠÙ',\n",
       " 'Ø§Ù„Ù…Ù†ÙŠØ¹',\n",
       " 'ÙˆØ¥Ù‚Ø§Ù…Ø©',\n",
       " 'Ø±Ø¤Ø³',\n",
       " 'Ø¬Ø³ÙˆØ±',\n",
       " 'Ù„Ù‡Ø§',\n",
       " 'Ø¨Ø¹Ø¯',\n",
       " 'Ø£Ù†',\n",
       " 'Ø£ÙÙ‚Ø¯Øª',\n",
       " 'Ø§Ù„Ø¹Ø¯Ùˆ',\n",
       " 'ØªÙˆØ§Ø²Ù†Ù‡',\n",
       " 'ÙÙ‰',\n",
       " '6',\n",
       " 'Ø³Ø§Ø¹Ø§Øª',\n",
       " 'Ù„Ù‚Ø¯',\n",
       " 'ÙƒØ§Ù†Øª',\n",
       " 'Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø©',\n",
       " 'ÙƒØ¨ÙŠØ±Ø©',\n",
       " 'ÙˆØ§Ù„ØªØ¶Ø­ÙŠØ§Øª',\n",
       " 'Ø¹Ø¸ÙŠÙ…Ø©',\n",
       " 'Ù„Ù…Ø¹Ø±ÙƒØ©',\n",
       " 'Ù¦',\n",
       " 'Ø£ÙƒØªÙˆØ¨Ø±',\n",
       " 'Ø®Ù„Ø§Ù„',\n",
       " 'Ø§Ù„Ø³Ø§Ø¹Ø§Øª',\n",
       " 'Ø§Ù„Ø³Øª',\n",
       " 'Ø§Ù„Ø£ÙˆÙ„ÙŠ',\n",
       " 'Ù…Ù†',\n",
       " 'Ø­Ø±Ø¨Ù†Ø§',\n",
       " 'ÙƒØ§Ù†Øª',\n",
       " 'Ù‡Ø§Ø¦Ù„Ø©',\n",
       " 'ÙÙ‚Ø¯',\n",
       " 'Ø§Ù„Ø¹Ø¯Ùˆ',\n",
       " 'ØªÙˆØ§Ø²Ù†Ù‡',\n",
       " 'Ø¥Ù„ÙŠ',\n",
       " 'Ù‡Ø°Ù‡',\n",
       " 'Ø§Ù„Ù„Ø­Ø¸Ø©']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **ğŸ“ Q2: Implement Sentence Segmentation Using `nltk`** âœ‚ï¸ğŸ“–\n",
    "\n",
    "### **ğŸ”¹ Task Description**\n",
    "Sentence **segmentation** is the process of dividing a paragraph into **individual sentences**. In this task, you will use **NLTKâ€™s Punkt Sentence Tokenizer** to segment text into sentences.\n"
   ],
   "id": "2764c6cd05e23610"
  },
  {
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:16.965317Z",
     "start_time": "2025-02-15T22:50:16.960604Z"
    }
   },
   "cell_type": "code",
   "source": "par=\" ÙÙŠ Ù†ÙØ³ Ø§Ù„Ù…Ø³Ø§Ø­Ø©ØŒ Ù„Ù‚Ø¯ ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ù…Ù† Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨Ù‰ØŒ Ø­ÙŠØ« ÙŠÙ…ÙƒÙ†Ùƒ Ø£Ù† ØªÙˆÙ„Ø¯ Ù…Ø«Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ø£Ùˆ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø£Ø®Ø±Ù‰ Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„ØªÙ‰ ÙŠÙˆÙ„Ø¯Ù‡Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚. Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¹Ø¯Ø¯ Ø£ÙƒØ¨Ø± Ù…Ù† Ø§Ù„ÙÙ‚Ø±Ø§Øª ÙŠØªÙŠØ­ Ù„Ùƒ Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„ÙÙ‚Ø±Ø§Øª ÙƒÙ…Ø§ ØªØ±ÙŠØ¯ØŒ Ø§Ù„Ù†Øµ Ù„Ù† ÙŠØ¨Ø¯Ùˆ Ù…Ù‚Ø³Ù…Ø§ ÙˆÙ„Ø§ ÙŠØ­ÙˆÙŠ Ø£Ø®Ø·Ø§Ø¡ Ù„ØºÙˆÙŠØ©ØŒ Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨Ù‰ Ù…ÙÙŠØ¯ Ù„Ù…ØµÙ…Ù…ÙŠ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø¹Ù„Ù‰ ÙˆØ¬Ù‡ Ø§Ù„Ø®ØµÙˆØµØŒ Ø­ÙŠØ« ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙÙ‰ ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ø£Ø­ÙŠØ§Ù† Ø£Ù† ÙŠØ·Ù„Ø¹ Ø¹Ù„Ù‰ ØµÙˆØ±Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù„ØªØµÙ…ÙŠÙ… Ø§Ù„Ù…ÙˆÙ‚Ø¹.ÙˆÙ…Ù† Ù‡Ù†Ø§ ÙˆØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØµÙ…Ù… Ø£Ù† ÙŠØ¶Ø¹ Ù†ØµÙˆØµØ§ Ù…Ø¤Ù‚ØªØ© Ø¹Ù„Ù‰ Ø§Ù„ØªØµÙ…ÙŠÙ… Ù„ÙŠØ¸Ù‡Ø± Ù„Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„Ø§Ù‹ØŒØ¯ÙˆØ± Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨Ù‰ Ø£Ù† ÙŠÙˆÙØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…ØµÙ…Ù… Ø¹Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Øµ Ø¨Ø¯ÙŠÙ„ Ù„Ø§ Ø¹Ù„Ø§Ù‚Ø© Ù„Ù‡ Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ \"\n",
   "id": "44222fab72cf55f2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.008782Z",
     "start_time": "2025-02-15T22:50:17.001795Z"
    }
   },
   "source": [
    "#============= Segmentation ==========\n",
    "def segment(text):\n",
    "    segmenter = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "    sents = segmenter.tokenize(text)\n",
    "    for sent in sents:\n",
    "        print(sent)\n",
    "        print('-'*30)\n",
    "\n",
    "    print(f\"Number of sentences: {len(sents)}\")\n"
   ],
   "id": "d6b54832b4248316",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.070685Z",
     "start_time": "2025-02-15T22:50:17.035725Z"
    }
   },
   "source": "segment(par)",
   "id": "5d1ffa9478b92d17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ÙÙŠ Ù†ÙØ³ Ø§Ù„Ù…Ø³Ø§Ø­Ø©ØŒ Ù„Ù‚Ø¯ ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ù…Ù† Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨Ù‰ØŒ Ø­ÙŠØ« ÙŠÙ…ÙƒÙ†Ùƒ Ø£Ù† ØªÙˆÙ„Ø¯ Ù…Ø«Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ø£Ùˆ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø£Ø®Ø±Ù‰ Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„ØªÙ‰ ÙŠÙˆÙ„Ø¯Ù‡Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.\n",
      "------------------------------\n",
      "Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¹Ø¯Ø¯ Ø£ÙƒØ¨Ø± Ù…Ù† Ø§Ù„ÙÙ‚Ø±Ø§Øª ÙŠØªÙŠØ­ Ù„Ùƒ Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„ÙÙ‚Ø±Ø§Øª ÙƒÙ…Ø§ ØªØ±ÙŠØ¯ØŒ Ø§Ù„Ù†Øµ Ù„Ù† ÙŠØ¨Ø¯Ùˆ Ù…Ù‚Ø³Ù…Ø§ ÙˆÙ„Ø§ ÙŠØ­ÙˆÙŠ Ø£Ø®Ø·Ø§Ø¡ Ù„ØºÙˆÙŠØ©ØŒ Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨Ù‰ Ù…ÙÙŠØ¯ Ù„Ù…ØµÙ…Ù…ÙŠ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø¹Ù„Ù‰ ÙˆØ¬Ù‡ Ø§Ù„Ø®ØµÙˆØµØŒ Ø­ÙŠØ« ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙÙ‰ ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ø£Ø­ÙŠØ§Ù† Ø£Ù† ÙŠØ·Ù„Ø¹ Ø¹Ù„Ù‰ ØµÙˆØ±Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù„ØªØµÙ…ÙŠÙ… Ø§Ù„Ù…ÙˆÙ‚Ø¹.ÙˆÙ…Ù† Ù‡Ù†Ø§ ÙˆØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØµÙ…Ù… Ø£Ù† ÙŠØ¶Ø¹ Ù†ØµÙˆØµØ§ Ù…Ø¤Ù‚ØªØ© Ø¹Ù„Ù‰ Ø§Ù„ØªØµÙ…ÙŠÙ… Ù„ÙŠØ¸Ù‡Ø± Ù„Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„Ø§Ù‹ØŒØ¯ÙˆØ± Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨Ù‰ Ø£Ù† ÙŠÙˆÙØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…ØµÙ…Ù… Ø¹Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Øµ Ø¨Ø¯ÙŠÙ„ Ù„Ø§ Ø¹Ù„Ø§Ù‚Ø© Ù„Ù‡ Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹\n",
      "------------------------------\n",
      "Number of sentences: 2\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eca584be"
   },
   "source": [
    "---\n",
    "\n",
    "# **ğŸ“ Q3: Removing Stopwords Using `nltk.corpus`** ğŸš€\n",
    "\n",
    "### **ğŸ”¹ Task Description**\n",
    "Stopwords are **common words** (such as \"the\", \"and\", \"is\") that usually **do not add significant meaning** in text analysis. Using **NLTK's `corpus` module**, remove **stopwords** from the following Arabic sentences.\n",
    "\n",
    "---"
   ],
   "id": "ea6bd40b4d854bc7"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5924086",
    "outputId": "0b483f04-2cbe-4242-86ed-59501c75afc7",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.094216Z",
     "start_time": "2025-02-15T22:50:17.089647Z"
    }
   },
   "source": [
    "X = [\n",
    "    \"Ø£Ø­Ø¨ Ø§Ù„Ù…Ø´ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø·Ø¦\",\n",
    "    \"Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ø£ÙÙ„Ø§Ù…\",\n",
    "    \"Ø£Ø­Ø¨ Ø§Ù„Ø£ÙÙ„Ø§Ù…\",\n",
    "    \"Ø§Ù„Ù‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø¨Ø¹Ø©\",\n",
    "]"
   ],
   "id": "b8cf2af540482562",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3f975e7",
    "outputId": "b56d001b-38ae-490e-9622-5fe35f8c339d",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.113409Z",
     "start_time": "2025-02-15T22:50:17.108384Z"
    }
   },
   "source": [
    "Y = [\n",
    "    \"Ø§Ù„Ø´Ø§Ø·Ø¦ Ù„Ù„Ø¨Ø­Ø± Ù…Ø«Ù„ Ø§Ù„Ø³Ø­Ø§Ø¨Ø© Ù„Ù„Ø³Ù…Ø§Ø¡\",\n",
    "    \"Ø£Ø­Ø¨ Ø§Ù„Ø£ÙÙ„Ø§Ù…\",\n",
    "    \"Ø§Ù†Ø§ Ø§Ø­Ø¨ Ø§Ù„Ø§ÙÙ„Ø§Ù…\",\n",
    "    \"Ø§Ù„Ù‚Ø·Ø© ÙÙˆÙ‚ Ø§Ù„Ù‚Ø¨Ø¹Ø©\",\n",
    "]\n"
   ],
   "id": "1db6e943adb3f7ef",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98ce0ae1",
    "outputId": "1ba20a47-5dec-461a-cb81-5ea799467b2c",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.138049Z",
     "start_time": "2025-02-15T22:50:17.128873Z"
    }
   },
   "source": [
    "# print stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('arabic')\n",
    "print(f\"Stop words length : {len(stopwords)}\")\n",
    "print(f\"Example : {stopwords[0:10]}\")"
   ],
   "id": "890ec6a8e4ce83cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words length : 754\n",
      "Example : ['Ø¥Ø°', 'Ø¥Ø°Ø§', 'Ø¥Ø°Ù…Ø§', 'Ø¥Ø°Ù†', 'Ø£Ù', 'Ø£Ù‚Ù„', 'Ø£ÙƒØ«Ø±', 'Ø£Ù„Ø§', 'Ø¥Ù„Ø§', 'Ø§Ù„ØªÙŠ']\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.179926Z",
     "start_time": "2025-02-15T22:50:17.176031Z"
    }
   },
   "source": [
    "from nltk.tokenize import word_tokenize"
   ],
   "id": "b35c5b178696252",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ceef2c37",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.222530Z",
     "start_time": "2025-02-15T22:50:17.216821Z"
    }
   },
   "source": [
    "def RemoveStopwords(X,Y):\n",
    "    \n",
    "    print(\"X:\", X)\n",
    "    print(\"Y:\", Y)\n",
    "    \n",
    "    # tokenization\n",
    "    X_list = word_tokenize(X)\n",
    "    Y_list = word_tokenize(Y)\n",
    "    \n",
    "    # remove stop words from the string\n",
    "    X_set = {w for w in X_list if not w in stopwords}\n",
    "    Y_set = {w for w in Y_list if not w in stopwords}\n",
    "\n",
    "    # X_set = {w for w in X_list if not w in stopwords}\n",
    "    # Is the same as ---------------------------------------------------\n",
    "    # for w in X_list:  # Loop through each word in X_list\n",
    "    #     if w not in stopwords:  # If the word is NOT a stopword\n",
    "    #         X_set.add(w)  # Add it to the set\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"X_set:\", X_set)\n",
    "    print(\"Y_set:\", Y_set)"
   ],
   "id": "9b842e35ea9dfd4a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.245086Z",
     "start_time": "2025-02-15T22:50:17.238493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for item in zip(X,Y):\n",
    "    print(item)"
   ],
   "id": "7898463bf13e76e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ø£Ø­Ø¨ Ø§Ù„Ù…Ø´ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø·Ø¦', 'Ø§Ù„Ø´Ø§Ø·Ø¦ Ù„Ù„Ø¨Ø­Ø± Ù…Ø«Ù„ Ø§Ù„Ø³Ø­Ø§Ø¨Ø© Ù„Ù„Ø³Ù…Ø§Ø¡')\n",
      "('Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ø£ÙÙ„Ø§Ù…', 'Ø£Ø­Ø¨ Ø§Ù„Ø£ÙÙ„Ø§Ù…')\n",
      "('Ø£Ø­Ø¨ Ø§Ù„Ø£ÙÙ„Ø§Ù…', 'Ø§Ù†Ø§ Ø§Ø­Ø¨ Ø§Ù„Ø§ÙÙ„Ø§Ù…')\n",
      "('Ø§Ù„Ù‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø¨Ø¹Ø©', 'Ø§Ù„Ù‚Ø·Ø© ÙÙˆÙ‚ Ø§Ù„Ù‚Ø¨Ø¹Ø©')\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.272321Z",
     "start_time": "2025-02-15T22:50:17.266068Z"
    }
   },
   "source": [
    "for x,y in zip(X, Y):\n",
    "    RemoveStopwords(x, y)\n",
    "    print('-'*30)"
   ],
   "id": "44d2b89e66dd7f21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: Ø£Ø­Ø¨ Ø§Ù„Ù…Ø´ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø§Ø·Ø¦\n",
      "Y: Ø§Ù„Ø´Ø§Ø·Ø¦ Ù„Ù„Ø¨Ø­Ø± Ù…Ø«Ù„ Ø§Ù„Ø³Ø­Ø§Ø¨Ø© Ù„Ù„Ø³Ù…Ø§Ø¡\n",
      "X_set: {'Ø§Ù„Ù…Ø´ÙŠ', 'Ø§Ù„Ø´Ø§Ø·Ø¦', 'Ø£Ø­Ø¨'}\n",
      "Y_set: {'Ù„Ù„Ø³Ù…Ø§Ø¡', 'Ø§Ù„Ø³Ø­Ø§Ø¨Ø©', 'Ø§Ù„Ø´Ø§Ø·Ø¦', 'Ù„Ù„Ø¨Ø­Ø±'}\n",
      "------------------------------\n",
      "X: Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ø£ÙÙ„Ø§Ù…\n",
      "Y: Ø£Ø­Ø¨ Ø§Ù„Ø£ÙÙ„Ø§Ù…\n",
      "X_set: {'Ø£Ø­Ø¨', 'Ø§Ù„Ø£ÙÙ„Ø§Ù…'}\n",
      "Y_set: {'Ø£Ø­Ø¨', 'Ø§Ù„Ø£ÙÙ„Ø§Ù…'}\n",
      "------------------------------\n",
      "X: Ø£Ø­Ø¨ Ø§Ù„Ø£ÙÙ„Ø§Ù…\n",
      "Y: Ø§Ù†Ø§ Ø§Ø­Ø¨ Ø§Ù„Ø§ÙÙ„Ø§Ù…\n",
      "X_set: {'Ø£Ø­Ø¨', 'Ø§Ù„Ø£ÙÙ„Ø§Ù…'}\n",
      "Y_set: {'Ø§Ù„Ø§ÙÙ„Ø§Ù…', 'Ø§Ù†Ø§', 'Ø§Ø­Ø¨'}\n",
      "------------------------------\n",
      "X: Ø§Ù„Ù‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø¨Ø¹Ø©\n",
      "Y: Ø§Ù„Ù‚Ø·Ø© ÙÙˆÙ‚ Ø§Ù„Ù‚Ø¨Ø¹Ø©\n",
      "X_set: {'Ø§Ù„Ù‚Ø·', 'Ø§Ù„Ù‚Ø¨Ø¹Ø©'}\n",
      "Y_set: {'Ø§Ù„Ù‚Ø·Ø©', 'Ø§Ù„Ù‚Ø¨Ø¹Ø©'}\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### **ğŸ”¹ What is an Arabic Corpus?**\n",
    "An **Arabic corpus** (plural: **corpora**) is a **large collection of Arabic text** used for **training and evaluating Natural Language Processing (NLP) models**. These corpora help in tasks such as **tokenization, part-of-speech tagging, named entity recognition, sentiment analysis, and machine translation**.\n",
    "\n",
    "---"
   ],
   "id": "dd074e9b03007ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.446439Z",
     "start_time": "2025-02-15T22:50:17.442458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Where to put your corpus ?  \n",
    "# nltk.data.path"
   ],
   "id": "5fe82c2ad106fa0d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.532766Z",
     "start_time": "2025-02-15T22:50:17.528325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading it\n",
    "# nltk.data.load('PATH')"
   ],
   "id": "bd5551e2f1dfe9c6",
   "outputs": [],
   "execution_count": 19
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MOhamed ElMesawy_Lab_01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
