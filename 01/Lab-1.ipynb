{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# โจ Text Normalization Tasks ๐\n",
    "\n",
    "Text normalization is an essential **preprocessing step** in NLP. It includes:\n",
    "\n",
    "- ๐น **Tokenization**: Splitting text into words or subwords.  \n",
    "- ๐น **Segmentation**: Dividing text into meaningful units such as sentences or phrases.  \n",
    "- ๐น **Stemming**: Reducing words to their root form by removing affixes.  \n",
    "- ๐น **Lemmatization**: Converting words to their base dictionary form.  "
   ],
   "id": "7aca1de6d065c151"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **๐น Arabic Text**\n",
    "> ุฅู ุงูุชุงุฑูุฎ ุงูุนุณูุฑู ุณูู ูุชููู ุทูููุงู ุฃูุงู ุนูููุฉ ููู **ูฆ ุฃูุชูุจุฑ ูกูฉูงูฃ**ุ ููุณุช ุฃุชุฌุงูุฒ ุฅุฐุง ููุช ุฃู ุงูุชุงุฑูุฎ ุงูุนุณูุฑู ุณูู ูุชููู ุทูููุงู ุจุงููุญุต ูุงูุฏุฑุณ ุฃูุงู ุนูููุฉ ููู **ูฆ ุฃูุชูุจุฑ ุณูุฉ ูงูฃ**ุ ุญูู ุชูููุช ุงูููุงุช ุงููุณูุญุฉ ุงููุตุฑูุฉ ูู ุงูุชุญุงู ูุงูุน **ููุงุฉ ุงูุณููุณ** ุงูุตุนุจ ูุงุฌุชูุงุฒ **ุฎุท ุจุงุฑููู** ุงููููุน ูุฅูุงูุฉ ุฑุคูุณ ุฌุณูุฑ ููุง ุจุนุฏ ุฃู ุฃููุฏุช ุงูุนุฏู ุชูุงุฒูู ูู **6 ุณุงุนุงุช**ุ ููุฏ ูุงูุช ุงููุฎุงุทุฑุฉ ูุจูุฑุฉ ูุงูุชุถุญูุงุช ุนุธููุฉ ููุนุฑูุฉ **ูฆ ุฃูุชูุจุฑ** ุฎูุงู **ุงูุณุงุนุงุช ุงูุณุช ุงูุฃููู** ูู ุญุฑุจูุง ูุงูุช ูุงุฆูุฉ ููุฏ ุงูุนุฏู ุชูุงุฒูู ุฅูู ูุฐู ุงููุญุธุฉ.\n"
   ],
   "id": "3dfcc28372cdb957"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aabe4a5c"
   },
   "source": [
    "## **Q1: Tokenization Using Three Methods**\n",
    "\n",
    "Tokenization is the process of breaking down a **phrase, sentence, paragraph, or entire document** into smaller units called **tokens**. These tokens can be individual **words, subwords, or punctuation marks**.\n",
    "\n",
    "For this task, apply **three different tokenization methods**:\n",
    "1. **Two traditional Python-based methods**\n",
    "2. **One NLP library-based method**\n"
   ],
   "id": "325fb1b363c0fdf9"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "9870a17a",
    "outputId": "c155c580-d966-408c-fc1f-cf6f89dc295e",
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:15.703569Z",
     "start_time": "2025-02-15T22:50:15.694255Z"
    }
   },
   "source": "text = ' ุฅู ุงูุชุงุฑูุฎ ุงูุนุณูุฑู ุณูู ูุชููู ุทูููุงู ุฃูุงู ุนูููุฉ ููู ูฆ ุฃูุชูุจุฑ ูกูฉูงูฃ, ููุณุช ุงุชุฌุงูุฒ ุงุฐุง ููุช ุฃู ุงูุชุงุฑูุฎ ุงูุนุณูุฑู ุณูู ูุชููู ุทูููุงู ุจุงููุญุต ูุงูุฏุฑุณ ุฃูุงู ุนูููุฉ ููู ูฆ ุฃูุชูุจุฑ ุณูุฉ ูงูฃ, ุญูู ุชูููุช ุงูููุงุช ุงููุณูุญุฉ ุงููุตุฑูุฉ ูู ุฅูุชุญุงู ูุงูุน ููุงุฉ ุงูุณููุณ ุงูุตุนุจ ูุงุฌุชูุงุฒ ุฎุท ุจุงุฑููู ุงููููุน ูุฅูุงูุฉ ุฑุคุณ ุฌุณูุฑ ููุง ุจุนุฏ ุฃู ุฃููุฏุช ุงูุนุฏู ุชูุงุฒูู ูู 6 ุณุงุนุงุช, ููุฏ ูุงูุช ุงููุฎุงุทุฑุฉ ูุจูุฑุฉ ูุงูุชุถุญูุงุช ุนุธููุฉ ููุนุฑูุฉ ูฆ ุฃูุชูุจุฑ ุฎูุงู ุงูุณุงุนุงุช ุงูุณุช ุงูุฃููู ูู ุญุฑุจูุง ูุงูุช ูุงุฆูุฉ ููุฏ ุงูุนุฏู ุชูุงุฒูู ุฅูู ูุฐู ุงููุญุธุฉ'",
   "id": "e55973fec6b869fe",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "cc744678",
    "outputId": "1e88e2a2-2a40-4aa5-c3a4-3c3a80bad2be",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:15.732963Z",
     "start_time": "2025-02-15T22:50:15.723522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocessing\n",
    "import re\n",
    "pre_processed = re.sub(r'[^\\w\\s]', '', text) # Keep only whitespaces & words\n",
    "pre_processed"
   ],
   "id": "165b13ee7a308e88",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ุฅู ุงูุชุงุฑูุฎ ุงูุนุณูุฑู ุณูู ูุชููู ุทูููุง ุฃูุงู ุนูููุฉ ููู ูฆ ุฃูุชูุจุฑ ูกูฉูงูฃ ููุณุช ุงุชุฌุงูุฒ ุงุฐุง ููุช ุฃู ุงูุชุงุฑูุฎ ุงูุนุณูุฑู ุณูู ูุชููู ุทูููุง ุจุงููุญุต ูุงูุฏุฑุณ ุฃูุงู ุนูููุฉ ููู ูฆ ุฃูุชูุจุฑ ุณูุฉ ูงูฃ ุญูู ุชูููุช ุงูููุงุช ุงููุณูุญุฉ ุงููุตุฑูุฉ ูู ุฅูุชุญุงู ูุงูุน ููุงุฉ ุงูุณููุณ ุงูุตุนุจ ูุงุฌุชูุงุฒ ุฎุท ุจุงุฑููู ุงููููุน ูุฅูุงูุฉ ุฑุคุณ ุฌุณูุฑ ููุง ุจุนุฏ ุฃู ุฃููุฏุช ุงูุนุฏู ุชูุงุฒูู ูู 6 ุณุงุนุงุช ููุฏ ูุงูุช ุงููุฎุงุทุฑุฉ ูุจูุฑุฉ ูุงูุชุถุญูุงุช ุนุธููุฉ ููุนุฑูุฉ ูฆ ุฃูุชูุจุฑ ุฎูุงู ุงูุณุงุนุงุช ุงูุณุช ุงูุฃููู ูู ุญุฑุจูุง ูุงูุช ูุงุฆูุฉ ููุฏ ุงูุนุฏู ุชูุงุฒูู ุฅูู ูุฐู ุงููุญุธุฉ'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d02ed2f"
   },
   "source": [
    "## **๐ธ Tokenization Methods**\n",
    "1๏ธโฃ **Pythonโs Built-in `split()` Method** (Basic, whitespace-based tokenization)  \n",
    "2๏ธโฃ **Regular Expressions (`re.findall(PATTERN, TEXT)`,`re.split()`)**  \n",
    "3๏ธโฃ **NLP Library (`nltk.word_tokenize` for English, `Farasa` for Arabic)** (Context-aware, language-specific tokenization)\n"
   ],
   "id": "90b93299e4334fb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **1๏ธโฃ Using Pythonโs `split()` Method (Traditional Method)**\n",
    "Python's built-in `split()` method can be used to tokenize text **based on spaces**. However, it does not handle punctuation or special cases."
   ],
   "id": "3f28a8b0bc968c87"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9ff7cc5",
    "outputId": "a96952f7-cc17-4a4d-d363-ce0e3c25efad",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:15.989553Z",
     "start_time": "2025-02-15T22:50:15.981097Z"
    }
   },
   "source": [
    "tokens_1 = pre_processed.split()\n",
    "tokens_1"
   ],
   "id": "8f5d0b349acc3e26",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ุฅู',\n",
       " 'ุงูุชุงุฑูุฎ',\n",
       " 'ุงูุนุณูุฑู',\n",
       " 'ุณูู',\n",
       " 'ูุชููู',\n",
       " 'ุทูููุง',\n",
       " 'ุฃูุงู',\n",
       " 'ุนูููุฉ',\n",
       " 'ููู',\n",
       " 'ูฆ',\n",
       " 'ุฃูุชูุจุฑ',\n",
       " 'ูกูฉูงูฃ',\n",
       " 'ููุณุช',\n",
       " 'ุงุชุฌุงูุฒ',\n",
       " 'ุงุฐุง',\n",
       " 'ููุช',\n",
       " 'ุฃู',\n",
       " 'ุงูุชุงุฑูุฎ',\n",
       " 'ุงูุนุณูุฑู',\n",
       " 'ุณูู',\n",
       " 'ูุชููู',\n",
       " 'ุทูููุง',\n",
       " 'ุจุงููุญุต',\n",
       " 'ูุงูุฏุฑุณ',\n",
       " 'ุฃูุงู',\n",
       " 'ุนูููุฉ',\n",
       " 'ููู',\n",
       " 'ูฆ',\n",
       " 'ุฃูุชูุจุฑ',\n",
       " 'ุณูุฉ',\n",
       " 'ูงูฃ',\n",
       " 'ุญูู',\n",
       " 'ุชูููุช',\n",
       " 'ุงูููุงุช',\n",
       " 'ุงููุณูุญุฉ',\n",
       " 'ุงููุตุฑูุฉ',\n",
       " 'ูู',\n",
       " 'ุฅูุชุญุงู',\n",
       " 'ูุงูุน',\n",
       " 'ููุงุฉ',\n",
       " 'ุงูุณููุณ',\n",
       " 'ุงูุตุนุจ',\n",
       " 'ูุงุฌุชูุงุฒ',\n",
       " 'ุฎุท',\n",
       " 'ุจุงุฑููู',\n",
       " 'ุงููููุน',\n",
       " 'ูุฅูุงูุฉ',\n",
       " 'ุฑุคุณ',\n",
       " 'ุฌุณูุฑ',\n",
       " 'ููุง',\n",
       " 'ุจุนุฏ',\n",
       " 'ุฃู',\n",
       " 'ุฃููุฏุช',\n",
       " 'ุงูุนุฏู',\n",
       " 'ุชูุงุฒูู',\n",
       " 'ูู',\n",
       " '6',\n",
       " 'ุณุงุนุงุช',\n",
       " 'ููุฏ',\n",
       " 'ูุงูุช',\n",
       " 'ุงููุฎุงุทุฑุฉ',\n",
       " 'ูุจูุฑุฉ',\n",
       " 'ูุงูุชุถุญูุงุช',\n",
       " 'ุนุธููุฉ',\n",
       " 'ููุนุฑูุฉ',\n",
       " 'ูฆ',\n",
       " 'ุฃูุชูุจุฑ',\n",
       " 'ุฎูุงู',\n",
       " 'ุงูุณุงุนุงุช',\n",
       " 'ุงูุณุช',\n",
       " 'ุงูุฃููู',\n",
       " 'ูู',\n",
       " 'ุญุฑุจูุง',\n",
       " 'ูุงูุช',\n",
       " 'ูุงุฆูุฉ',\n",
       " 'ููุฏ',\n",
       " 'ุงูุนุฏู',\n",
       " 'ุชูุงุฒูู',\n",
       " 'ุฅูู',\n",
       " 'ูุฐู',\n",
       " 'ุงููุญุธุฉ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f68764a1"
   },
   "source": [
    "## **2๏ธโฃ Using Regular Expressions**\n",
    "We can use **regular expressions (`re`)** to split text.\n"
   ],
   "id": "d572753a42560838"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8a9333a5",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:16.031445Z",
     "start_time": "2025-02-15T22:50:16.024047Z"
    }
   },
   "source": [
    "PATTERN = r\"\\w+\"\n",
    "tokens_2 = re.findall(PATTERN, pre_processed)\n",
    "tokens_2"
   ],
   "id": "4c88f639656460fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ุฅู',\n",
       " 'ุงูุชุงุฑูุฎ',\n",
       " 'ุงูุนุณูุฑู',\n",
       " 'ุณูู',\n",
       " 'ูุชููู',\n",
       " 'ุทูููุง',\n",
       " 'ุฃูุงู',\n",
       " 'ุนูููุฉ',\n",
       " 'ููู',\n",
       " 'ูฆ',\n",
       " 'ุฃูุชูุจุฑ',\n",
       " 'ูกูฉูงูฃ',\n",
       " 'ููุณุช',\n",
       " 'ุงุชุฌุงูุฒ',\n",
       " 'ุงุฐุง',\n",
       " 'ููุช',\n",
       " 'ุฃู',\n",
       " 'ุงูุชุงุฑูุฎ',\n",
       " 'ุงูุนุณูุฑู',\n",
       " 'ุณูู',\n",
       " 'ูุชููู',\n",
       " 'ุทูููุง',\n",
       " 'ุจุงููุญุต',\n",
       " 'ูุงูุฏุฑุณ',\n",
       " 'ุฃูุงู',\n",
       " 'ุนูููุฉ',\n",
       " 'ููู',\n",
       " 'ูฆ',\n",
       " 'ุฃูุชูุจุฑ',\n",
       " 'ุณูุฉ',\n",
       " 'ูงูฃ',\n",
       " 'ุญูู',\n",
       " 'ุชูููุช',\n",
       " 'ุงูููุงุช',\n",
       " 'ุงููุณูุญุฉ',\n",
       " 'ุงููุตุฑูุฉ',\n",
       " 'ูู',\n",
       " 'ุฅูุชุญุงู',\n",
       " 'ูุงูุน',\n",
       " 'ููุงุฉ',\n",
       " 'ุงูุณููุณ',\n",
       " 'ุงูุตุนุจ',\n",
       " 'ูุงุฌุชูุงุฒ',\n",
       " 'ุฎุท',\n",
       " 'ุจุงุฑููู',\n",
       " 'ุงููููุน',\n",
       " 'ูุฅูุงูุฉ',\n",
       " 'ุฑุคุณ',\n",
       " 'ุฌุณูุฑ',\n",
       " 'ููุง',\n",
       " 'ุจุนุฏ',\n",
       " 'ุฃู',\n",
       " 'ุฃููุฏุช',\n",
       " 'ุงูุนุฏู',\n",
       " 'ุชูุงุฒูู',\n",
       " 'ูู',\n",
       " '6',\n",
       " 'ุณุงุนุงุช',\n",
       " 'ููุฏ',\n",
       " 'ูุงูุช',\n",
       " 'ุงููุฎุงุทุฑุฉ',\n",
       " 'ูุจูุฑุฉ',\n",
       " 'ูุงูุชุถุญูุงุช',\n",
       " 'ุนุธููุฉ',\n",
       " 'ููุนุฑูุฉ',\n",
       " 'ูฆ',\n",
       " 'ุฃูุชูุจุฑ',\n",
       " 'ุฎูุงู',\n",
       " 'ุงูุณุงุนุงุช',\n",
       " 'ุงูุณุช',\n",
       " 'ุงูุฃููู',\n",
       " 'ูู',\n",
       " 'ุญุฑุจูุง',\n",
       " 'ูุงูุช',\n",
       " 'ูุงุฆูุฉ',\n",
       " 'ููุฏ',\n",
       " 'ุงูุนุฏู',\n",
       " 'ุชูุงุฒูู',\n",
       " 'ุฅูู',\n",
       " 'ูุฐู',\n",
       " 'ุงููุญุธุฉ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cd43a3e"
   },
   "source": [
    "## **3๏ธโฃ Using NLP Libraries (`nltk.word_tokenize`)**\n",
    "The **NLTK tokenizer** provides a **smart way** to split words while handling punctuation **and contractions** properly."
   ],
   "id": "81b525c81f541758"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cfab0cc1",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:16.835764Z",
     "start_time": "2025-02-15T22:50:16.100666Z"
    }
   },
   "source": [
    "import nltk\n",
    "# Punkt is a pretrained model that helps in splitting text into sentences and words.\n",
    "# It supports multiple languages, including English and Arabic.\n",
    "# The \"punkt\" package consists in unsafe pickles, so it is deprecated and not used in NLTK 3.8.2. It is replaced by \"punkt_tab\".\n",
    "\n",
    "# nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ],
   "id": "135a6e89a98b1543",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mmaba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mmaba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8jCr3eI_ZNvG",
    "outputId": "44ab8cab-4383-4527-a57e-d6dac7379cb8",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:16.856275Z",
     "start_time": "2025-02-15T22:50:16.851956Z"
    }
   },
   "source": [
    "from nltk.tokenize import word_tokenize"
   ],
   "id": "8b637d53abe206a3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "220930ec",
    "outputId": "87c67cef-fe3e-469d-8be6-bc66cd9d2b36",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:16.933669Z",
     "start_time": "2025-02-15T22:50:16.893221Z"
    }
   },
   "source": [
    "tokens_3 = word_tokenize(pre_processed)\n",
    "tokens_3"
   ],
   "id": "34b0bee7b0185a6e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ุฅู',\n",
       " 'ุงูุชุงุฑูุฎ',\n",
       " 'ุงูุนุณูุฑู',\n",
       " 'ุณูู',\n",
       " 'ูุชููู',\n",
       " 'ุทูููุง',\n",
       " 'ุฃูุงู',\n",
       " 'ุนูููุฉ',\n",
       " 'ููู',\n",
       " 'ูฆ',\n",
       " 'ุฃูุชูุจุฑ',\n",
       " 'ูกูฉูงูฃ',\n",
       " 'ููุณุช',\n",
       " 'ุงุชุฌุงูุฒ',\n",
       " 'ุงุฐุง',\n",
       " 'ููุช',\n",
       " 'ุฃู',\n",
       " 'ุงูุชุงุฑูุฎ',\n",
       " 'ุงูุนุณูุฑู',\n",
       " 'ุณูู',\n",
       " 'ูุชููู',\n",
       " 'ุทูููุง',\n",
       " 'ุจุงููุญุต',\n",
       " 'ูุงูุฏุฑุณ',\n",
       " 'ุฃูุงู',\n",
       " 'ุนูููุฉ',\n",
       " 'ููู',\n",
       " 'ูฆ',\n",
       " 'ุฃูุชูุจุฑ',\n",
       " 'ุณูุฉ',\n",
       " 'ูงูฃ',\n",
       " 'ุญูู',\n",
       " 'ุชูููุช',\n",
       " 'ุงูููุงุช',\n",
       " 'ุงููุณูุญุฉ',\n",
       " 'ุงููุตุฑูุฉ',\n",
       " 'ูู',\n",
       " 'ุฅูุชุญุงู',\n",
       " 'ูุงูุน',\n",
       " 'ููุงุฉ',\n",
       " 'ุงูุณููุณ',\n",
       " 'ุงูุตุนุจ',\n",
       " 'ูุงุฌุชูุงุฒ',\n",
       " 'ุฎุท',\n",
       " 'ุจุงุฑููู',\n",
       " 'ุงููููุน',\n",
       " 'ูุฅูุงูุฉ',\n",
       " 'ุฑุคุณ',\n",
       " 'ุฌุณูุฑ',\n",
       " 'ููุง',\n",
       " 'ุจุนุฏ',\n",
       " 'ุฃู',\n",
       " 'ุฃููุฏุช',\n",
       " 'ุงูุนุฏู',\n",
       " 'ุชูุงุฒูู',\n",
       " 'ูู',\n",
       " '6',\n",
       " 'ุณุงุนุงุช',\n",
       " 'ููุฏ',\n",
       " 'ูุงูุช',\n",
       " 'ุงููุฎุงุทุฑุฉ',\n",
       " 'ูุจูุฑุฉ',\n",
       " 'ูุงูุชุถุญูุงุช',\n",
       " 'ุนุธููุฉ',\n",
       " 'ููุนุฑูุฉ',\n",
       " 'ูฆ',\n",
       " 'ุฃูุชูุจุฑ',\n",
       " 'ุฎูุงู',\n",
       " 'ุงูุณุงุนุงุช',\n",
       " 'ุงูุณุช',\n",
       " 'ุงูุฃููู',\n",
       " 'ูู',\n",
       " 'ุญุฑุจูุง',\n",
       " 'ูุงูุช',\n",
       " 'ูุงุฆูุฉ',\n",
       " 'ููุฏ',\n",
       " 'ุงูุนุฏู',\n",
       " 'ุชูุงุฒูู',\n",
       " 'ุฅูู',\n",
       " 'ูุฐู',\n",
       " 'ุงููุญุธุฉ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **๐ Q2: Implement Sentence Segmentation Using `nltk`** โ๏ธ๐\n",
    "\n",
    "### **๐น Task Description**\n",
    "Sentence **segmentation** is the process of dividing a paragraph into **individual sentences**. In this task, you will use **NLTKโs Punkt Sentence Tokenizer** to segment text into sentences.\n"
   ],
   "id": "2764c6cd05e23610"
  },
  {
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:16.965317Z",
     "start_time": "2025-02-15T22:50:16.960604Z"
    }
   },
   "cell_type": "code",
   "source": "par=\" ูู ููุณ ุงููุณุงุญุฉุ ููุฏ ุชู ุชูููุฏ ูุฐุง ุงููุต ูู ูููุฏ ุงููุต ุงูุนุฑุจูุ ุญูุซ ููููู ุฃู ุชููุฏ ูุซู ูุฐุง ุงููุต ุฃู ุงูุนุฏูุฏ ูู ุงููุตูุต ุงูุฃุฎุฑู ุฅุถุงูุฉ ุฅูู ุฒูุงุฏุฉ ุนุฏุฏ ุงูุญุฑูู ุงูุชู ูููุฏูุง ุงูุชุทุจูู. ุฅุฐุง ููุช ุชุญุชุงุฌ ุฅูู ุนุฏุฏ ุฃูุจุฑ ูู ุงูููุฑุงุช ูุชูุญ ูู ูููุฏ ุงููุต ุงูุนุฑุจู ุฒูุงุฏุฉ ุนุฏุฏ ุงูููุฑุงุช ููุง ุชุฑูุฏุ ุงููุต ูู ูุจุฏู ููุณูุง ููุง ูุญูู ุฃุฎุทุงุก ูุบููุฉุ ูููุฏ ุงููุต ุงูุนุฑุจู ูููุฏ ููุตููู ุงูููุงูุน ุนูู ูุฌู ุงูุฎุตูุตุ ุญูุซ ูุญุชุงุฌ ุงูุนููู ูู ูุซูุฑ ูู ุงูุฃุญูุงู ุฃู ูุทูุน ุนูู ุตูุฑุฉ ุญููููุฉ ูุชุตููู ุงููููุน.ููู ููุง ูุฌุจ ุนูู ุงููุตูู ุฃู ูุถุน ูุตูุตุง ูุคูุชุฉ ุนูู ุงูุชุตููู ููุธูุฑ ููุนููู ุงูุดูู ูุงููุงูุุฏูุฑ ูููุฏ ุงููุต ุงูุนุฑุจู ุฃู ูููุฑ ุนูู ุงููุตูู ุนูุงุก ุงูุจุญุซ ุนู ูุต ุจุฏูู ูุง ุนูุงูุฉ ูู ุจุงูููุถูุน \"\n",
   "id": "44222fab72cf55f2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.008782Z",
     "start_time": "2025-02-15T22:50:17.001795Z"
    }
   },
   "source": [
    "#============= Segmentation ==========\n",
    "def segment(text):\n",
    "    segmenter = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "    sents = segmenter.tokenize(text)\n",
    "    for sent in sents:\n",
    "        print(sent)\n",
    "        print('-'*30)\n",
    "\n",
    "    print(f\"Number of sentences: {len(sents)}\")\n"
   ],
   "id": "d6b54832b4248316",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.070685Z",
     "start_time": "2025-02-15T22:50:17.035725Z"
    }
   },
   "source": "segment(par)",
   "id": "5d1ffa9478b92d17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ูู ููุณ ุงููุณุงุญุฉุ ููุฏ ุชู ุชูููุฏ ูุฐุง ุงููุต ูู ูููุฏ ุงููุต ุงูุนุฑุจูุ ุญูุซ ููููู ุฃู ุชููุฏ ูุซู ูุฐุง ุงููุต ุฃู ุงูุนุฏูุฏ ูู ุงููุตูุต ุงูุฃุฎุฑู ุฅุถุงูุฉ ุฅูู ุฒูุงุฏุฉ ุนุฏุฏ ุงูุญุฑูู ุงูุชู ูููุฏูุง ุงูุชุทุจูู.\n",
      "------------------------------\n",
      "ุฅุฐุง ููุช ุชุญุชุงุฌ ุฅูู ุนุฏุฏ ุฃูุจุฑ ูู ุงูููุฑุงุช ูุชูุญ ูู ูููุฏ ุงููุต ุงูุนุฑุจู ุฒูุงุฏุฉ ุนุฏุฏ ุงูููุฑุงุช ููุง ุชุฑูุฏุ ุงููุต ูู ูุจุฏู ููุณูุง ููุง ูุญูู ุฃุฎุทุงุก ูุบููุฉุ ูููุฏ ุงููุต ุงูุนุฑุจู ูููุฏ ููุตููู ุงูููุงูุน ุนูู ูุฌู ุงูุฎุตูุตุ ุญูุซ ูุญุชุงุฌ ุงูุนููู ูู ูุซูุฑ ูู ุงูุฃุญูุงู ุฃู ูุทูุน ุนูู ุตูุฑุฉ ุญููููุฉ ูุชุตููู ุงููููุน.ููู ููุง ูุฌุจ ุนูู ุงููุตูู ุฃู ูุถุน ูุตูุตุง ูุคูุชุฉ ุนูู ุงูุชุตููู ููุธูุฑ ููุนููู ุงูุดูู ูุงููุงูุุฏูุฑ ูููุฏ ุงููุต ุงูุนุฑุจู ุฃู ูููุฑ ุนูู ุงููุตูู ุนูุงุก ุงูุจุญุซ ุนู ูุต ุจุฏูู ูุง ุนูุงูุฉ ูู ุจุงูููุถูุน\n",
      "------------------------------\n",
      "Number of sentences: 2\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eca584be"
   },
   "source": [
    "---\n",
    "\n",
    "# **๐ Q3: Removing Stopwords Using `nltk.corpus`** ๐\n",
    "\n",
    "### **๐น Task Description**\n",
    "Stopwords are **common words** (such as \"the\", \"and\", \"is\") that usually **do not add significant meaning** in text analysis. Using **NLTK's `corpus` module**, remove **stopwords** from the following Arabic sentences.\n",
    "\n",
    "---"
   ],
   "id": "ea6bd40b4d854bc7"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5924086",
    "outputId": "0b483f04-2cbe-4242-86ed-59501c75afc7",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.094216Z",
     "start_time": "2025-02-15T22:50:17.089647Z"
    }
   },
   "source": [
    "X = [\n",
    "    \"ุฃุญุจ ุงููุดู ุนูู ุงูุดุงุทุฆ\",\n",
    "    \"ุฃูุง ุฃุญุจ ุงูุฃููุงู\",\n",
    "    \"ุฃุญุจ ุงูุฃููุงู\",\n",
    "    \"ุงููุท ุนูู ุงููุจุนุฉ\",\n",
    "]"
   ],
   "id": "b8cf2af540482562",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3f975e7",
    "outputId": "b56d001b-38ae-490e-9622-5fe35f8c339d",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.113409Z",
     "start_time": "2025-02-15T22:50:17.108384Z"
    }
   },
   "source": [
    "Y = [\n",
    "    \"ุงูุดุงุทุฆ ููุจุญุฑ ูุซู ุงูุณุญุงุจุฉ ููุณูุงุก\",\n",
    "    \"ุฃุญุจ ุงูุฃููุงู\",\n",
    "    \"ุงูุง ุงุญุจ ุงูุงููุงู\",\n",
    "    \"ุงููุทุฉ ููู ุงููุจุนุฉ\",\n",
    "]\n"
   ],
   "id": "1db6e943adb3f7ef",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98ce0ae1",
    "outputId": "1ba20a47-5dec-461a-cb81-5ea799467b2c",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.138049Z",
     "start_time": "2025-02-15T22:50:17.128873Z"
    }
   },
   "source": [
    "# print stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('arabic')\n",
    "print(f\"Stop words length : {len(stopwords)}\")\n",
    "print(f\"Example : {stopwords[0:10]}\")"
   ],
   "id": "890ec6a8e4ce83cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words length : 754\n",
      "Example : ['ุฅุฐ', 'ุฅุฐุง', 'ุฅุฐูุง', 'ุฅุฐู', 'ุฃู', 'ุฃูู', 'ุฃูุซุฑ', 'ุฃูุง', 'ุฅูุง', 'ุงูุชู']\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.179926Z",
     "start_time": "2025-02-15T22:50:17.176031Z"
    }
   },
   "source": [
    "from nltk.tokenize import word_tokenize"
   ],
   "id": "b35c5b178696252",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ceef2c37",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.222530Z",
     "start_time": "2025-02-15T22:50:17.216821Z"
    }
   },
   "source": [
    "def RemoveStopwords(X,Y):\n",
    "    \n",
    "    print(\"X:\", X)\n",
    "    print(\"Y:\", Y)\n",
    "    \n",
    "    # tokenization\n",
    "    X_list = word_tokenize(X)\n",
    "    Y_list = word_tokenize(Y)\n",
    "    \n",
    "    # remove stop words from the string\n",
    "    X_set = {w for w in X_list if not w in stopwords}\n",
    "    Y_set = {w for w in Y_list if not w in stopwords}\n",
    "\n",
    "    # X_set = {w for w in X_list if not w in stopwords}\n",
    "    # Is the same as ---------------------------------------------------\n",
    "    # for w in X_list:  # Loop through each word in X_list\n",
    "    #     if w not in stopwords:  # If the word is NOT a stopword\n",
    "    #         X_set.add(w)  # Add it to the set\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"X_set:\", X_set)\n",
    "    print(\"Y_set:\", Y_set)"
   ],
   "id": "9b842e35ea9dfd4a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.245086Z",
     "start_time": "2025-02-15T22:50:17.238493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for item in zip(X,Y):\n",
    "    print(item)"
   ],
   "id": "7898463bf13e76e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ุฃุญุจ ุงููุดู ุนูู ุงูุดุงุทุฆ', 'ุงูุดุงุทุฆ ููุจุญุฑ ูุซู ุงูุณุญุงุจุฉ ููุณูุงุก')\n",
      "('ุฃูุง ุฃุญุจ ุงูุฃููุงู', 'ุฃุญุจ ุงูุฃููุงู')\n",
      "('ุฃุญุจ ุงูุฃููุงู', 'ุงูุง ุงุญุจ ุงูุงููุงู')\n",
      "('ุงููุท ุนูู ุงููุจุนุฉ', 'ุงููุทุฉ ููู ุงููุจุนุฉ')\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.272321Z",
     "start_time": "2025-02-15T22:50:17.266068Z"
    }
   },
   "source": [
    "for x,y in zip(X, Y):\n",
    "    RemoveStopwords(x, y)\n",
    "    print('-'*30)"
   ],
   "id": "44d2b89e66dd7f21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: ุฃุญุจ ุงููุดู ุนูู ุงูุดุงุทุฆ\n",
      "Y: ุงูุดุงุทุฆ ููุจุญุฑ ูุซู ุงูุณุญุงุจุฉ ููุณูุงุก\n",
      "X_set: {'ุงููุดู', 'ุงูุดุงุทุฆ', 'ุฃุญุจ'}\n",
      "Y_set: {'ููุณูุงุก', 'ุงูุณุญุงุจุฉ', 'ุงูุดุงุทุฆ', 'ููุจุญุฑ'}\n",
      "------------------------------\n",
      "X: ุฃูุง ุฃุญุจ ุงูุฃููุงู\n",
      "Y: ุฃุญุจ ุงูุฃููุงู\n",
      "X_set: {'ุฃุญุจ', 'ุงูุฃููุงู'}\n",
      "Y_set: {'ุฃุญุจ', 'ุงูุฃููุงู'}\n",
      "------------------------------\n",
      "X: ุฃุญุจ ุงูุฃููุงู\n",
      "Y: ุงูุง ุงุญุจ ุงูุงููุงู\n",
      "X_set: {'ุฃุญุจ', 'ุงูุฃููุงู'}\n",
      "Y_set: {'ุงูุงููุงู', 'ุงูุง', 'ุงุญุจ'}\n",
      "------------------------------\n",
      "X: ุงููุท ุนูู ุงููุจุนุฉ\n",
      "Y: ุงููุทุฉ ููู ุงููุจุนุฉ\n",
      "X_set: {'ุงููุท', 'ุงููุจุนุฉ'}\n",
      "Y_set: {'ุงููุทุฉ', 'ุงููุจุนุฉ'}\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### **๐น What is an Arabic Corpus?**\n",
    "An **Arabic corpus** (plural: **corpora**) is a **large collection of Arabic text** used for **training and evaluating Natural Language Processing (NLP) models**. These corpora help in tasks such as **tokenization, part-of-speech tagging, named entity recognition, sentiment analysis, and machine translation**.\n",
    "\n",
    "---"
   ],
   "id": "dd074e9b03007ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.446439Z",
     "start_time": "2025-02-15T22:50:17.442458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Where to put your corpus ?  \n",
    "# nltk.data.path"
   ],
   "id": "5fe82c2ad106fa0d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T22:50:17.532766Z",
     "start_time": "2025-02-15T22:50:17.528325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading it\n",
    "# nltk.data.load('PATH')"
   ],
   "id": "bd5551e2f1dfe9c6",
   "outputs": [],
   "execution_count": 19
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MOhamed ElMesawy_Lab_01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
